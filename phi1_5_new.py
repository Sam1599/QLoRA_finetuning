# -*- coding: utf-8 -*-
"""phi1_5_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a_XJbnu21xxmWw4YvNWfpz6fRpcc3q8h
"""

pip install transformers datasets accelerate peft torch

!pip install accelerate
!pip install -i https://pypi.org/simple/ bitsandbytes
pip wheel azure

"""Import all the necessary libraries"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
from datasets import load_dataset, Dataset

"""FSDP is used for fficiently parallelizing models by sharding model parameters and states across multiple GPUs or nodes."""

from accelerate import FullyShardedDataParallelPlugin, Accelerator
from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig

fsdp_plugin = FullyShardedDataParallelPlugin(
    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),
    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),
)

accelerator = Accelerator(fsdp_plugin=fsdp_plugin)

"""Loading the model and the tokenizer"""

base_model_id = "microsoft/phi-1_5"
model = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained(base_model_id)

"""The model is originally pretrained to generate python codes. An example is displayed below."""

inputs = tokenizer('''def factorial(n):
   """
   Print the factorial of the number n
   """''', return_tensors="pt", return_attention_mask=False)

outputs = model.generate(**inputs, max_length=200)
text = tokenizer.batch_decode(outputs)[0]
print(text)

"""The aim is to finetune the model using QLORA on a dataset similar to the datasets used in pretraining. The dataset chosen is DS-1000: A dataset containing DataScience problems in Pandas, NumPy, etc.. and their solutions."""

from datasets import load_dataset
ds1000 = load_dataset("xlangai/DS-1000", split="test")

# Initialize the tokenizer
base_model_id = "microsoft/phi-1_5"
tokenizer = AutoTokenizer.from_pretrained(base_model_id, padding= True, truncation=True)

if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

# Function to generate prompt from a data point
def generate_prompt(data_point):
    return f"""
    <Human>: {data_point['prompt']}
    <AI>: {data_point['reference_code']}
    """.strip()

# Function to generate and tokenize prompt
def generate_and_tokenize_prompt(data_point):
    full_prompt = generate_prompt(data_point)
    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)
    return tokenized_full_prompt

# Apply the function to the dataset
tokenized_ds1000 = ds1000.map(generate_and_tokenize_prompt)

# Print a sample to verify
print(tokenized_ds1000[0])

ds1000.column_names

tokenizer = AutoTokenizer.from_pretrained(base_model_id)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

model = AutoModelForCausalLM.from_pretrained(base_model_id)
model.resize_token_embeddings(len(tokenizer))

"""Loading the model in a memory efficient manner using bitsandbytesconfig"""

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)
model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

"""Prints the trainable parameters of the model"""

def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}")

"""Adds LORA configurations to the model for efficient memory usage"""

config = LoraConfig(
    r=32,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "dense"],
    modules_to_save=["lm_head", "embed_tokens"],
    bias="none",
    lora_dropout=0.1,
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, config)
print_trainable_parameters(model)

device = "cuda"
model.to(device)
if torch.cuda.device_count() > 1:
    model.is_parallelizable = True
    model.model_parallel = True

"""The training arguments and Trainer is set up"""

training_args = TrainingArguments(
    fp16=True,
    per_device_train_batch_size=2,
    #gradient_accumulation_steps=4,
    num_train_epochs=2,
    learning_rate=1e-4,
    output_dir="./phi1_5_finetuned/",
    logging_dir="./logs/",
    logging_steps=100,
    overwrite_output_dir=True,

)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_ds1000,
    data_collator=data_collator,
)
trainer.train()

trainer.save_model("./phi1_5_finetuned")

from transformers import pipeline
text_generator = pipeline("text-generation", model="/content/phi1_5_finetuned", tokenizer=tokenizer)
generated_text = text_generator("What is a Dataframe?", max_length=500, num_return_sequences=1)
print(generated_text)

